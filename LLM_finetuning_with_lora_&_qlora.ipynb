{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN2PAbBa84WVvg0e//7HgC4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sharma-himanshukumar/LLM_Learning/blob/main/LLM_finetuning_with_lora_%26_qlora.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LoRA and QLoRA\n",
        "\n",
        "## Overview\n",
        "\n",
        "**LoRA (Low-Rank Adaptation)** is a technique to fine-tune large pre-trained language models efficiently. Instead of updating all the parameters of the model during fine-tuning, LoRA introduces low-rank matrices to approximate the necessary updates. This reduces the computational cost and memory usage.\n",
        "\n",
        "**QLoRA (Quantized Low-Rank Adaptation)** extends LoRA by applying quantization to further reduce memory and computational requirements. In QLoRA, the weights and the low-rank matrices are quantized, typically to lower precision formats such as int8.\n",
        "\n",
        "## Concepts\n",
        "\n",
        "### LoRA (Low-Rank Adaptation)\n",
        "\n",
        "- Decomposes the weight updates into two low-rank matrices `A` and `B`.\n",
        "- Instead of updating the full weight matrix `W`, LoRA represents the update as `ΔW = A * B`.\n",
        "- During fine-tuning, the model updates `A` and `B` while keeping `W` fixed.\n",
        "\n",
        "### QLoRA (Quantized Low-Rank Adaptation)\n",
        "\n",
        "- Applies quantization to the low-rank matrices `A` and `B` to further reduce memory and computational requirements.\n",
        "- The quantization process involves mapping the floating-point values to a lower bit representation (e.g., int8).\n",
        "- Quantized inference uses the quantized versions of `A` and `B`.\n",
        "\n",
        "## Code Example\n",
        "\n",
        "The following code demonstrates the implementation of LoRA and QLoRA in PyTorch, and shows the difference in overall weights between the two methods.\n",
        "\n",
        "```python\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.quantization as quantization\n",
        "\n",
        "# Define a simple model with LoRA applied to one layer\n",
        "class LoRAModel(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim):\n",
        "        super(LoRAModel, self).__init__()\n",
        "        self.fc = nn.Linear(input_dim, hidden_dim)\n",
        "        self.lora_A = nn.Parameter(torch.randn(hidden_dim, 4))  # Low-rank matrix A\n",
        "        self.lora_B = nn.Parameter(torch.randn(4, input_dim))   # Low-rank matrix B\n",
        "    \n",
        "    def forward(self, x):\n",
        "        W = self.fc.weight\n",
        "        lora_update = torch.matmul(self.lora_A, self.lora_B)\n",
        "        updated_W = W + lora_update\n",
        "        x = torch.matmul(x, updated_W.T) + self.fc.bias\n",
        "        return x\n",
        "\n",
        "# Define quantization functions\n",
        "def quantize_tensor(tensor, num_bits=8):\n",
        "    scale = tensor.abs().max() / (2 ** (num_bits - 1) - 1)\n",
        "    quantized = torch.round(tensor / scale).int()\n",
        "    return quantized, scale\n",
        "\n",
        "def dequantize_tensor(quantized, scale):\n",
        "    return quantized.float() * scale\n",
        "\n",
        "# Instantiate the model\n",
        "input_dim = 10\n",
        "hidden_dim = 6\n",
        "model = LoRAModel(input_dim, hidden_dim)\n",
        "\n",
        "# Apply LoRA\n",
        "lora_A = model.lora_A\n",
        "lora_B = model.lora_B\n",
        "W = model.fc.weight\n",
        "lora_update = torch.matmul(lora_A, lora_B)\n",
        "updated_W_lora = W + lora_update\n",
        "\n",
        "# Apply QLoRA\n",
        "quantized_A, scale_A = quantize_tensor(lora_A)\n",
        "quantized_B, scale_B = quantize_tensor(lora_B)\n",
        "dequantized_A = dequantize_tensor(quantized_A, scale_A)\n",
        "dequantized_B = dequantize_tensor(quantized_B, scale_B)\n",
        "lora_update_quantized = torch.matmul(dequantized_A, dequantized_B)\n",
        "updated_W_qlora = W + lora_update_quantized\n",
        "\n",
        "# Print the differences between LoRA and QLoRA\n",
        "print(\"Original Weight Matrix W:\\n\", W)\n",
        "print(\"\\nLow-Rank Update ΔW (LoRA):\\n\", lora_update)\n",
        "print(\"\\nUpdated Weight Matrix W' (LoRA):\\n\", updated_W_lora)\n",
        "print(\"\\nLow-Rank Update ΔW (QLoRA):\\n\", lora_update_quantized)\n",
        "print(\"\\nUpdated Weight Matrix W' (QLoRA):\\n\", updated_W_qlora)\n",
        "```\n",
        "\n",
        "## Key Points\n",
        "\n",
        "- **LoRA**: Reduces the number of parameters by representing the updates as low-rank matrices `A` and `B`.\n",
        "- **QLoRA**: Quantizes the low-rank matrices to reduce memory usage and computational load.\n",
        "- **Quantization Methods**:\n",
        "  - **Static Quantization**: Quantizes weights and activations based on calibration data.\n",
        "  - **Dynamic Quantization**: Quantizes weights statically and activations dynamically during inference.\n",
        "  - **Quantization Aware Training (QAT)**: Simulates quantization during training to better adapt the model to lower precision.\n",
        "\n",
        "### Advantages of QLoRA\n",
        "\n",
        "- **Efficiency**: Reduces memory and computational requirements by using low-rank matrices and quantization.\n",
        "- **Performance**: Maintains a high level of performance by fine-tuning with low-rank adaptations and considering quantization effects during training.\n",
        "- **Scalability**: Allows scaling large models to even larger contexts by reducing the overhead of full precision computations.\n",
        "\n",
        "By integrating low-rank adaptations with quantization, QLoRA provides a powerful approach to efficiently fine-tune and deploy large language models.\n"
      ],
      "metadata": {
        "id": "KsNbqg6jV9yG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.quantization as quantization\n",
        "\n",
        "# Define a simple model with LoRA applied to one layer\n",
        "class LoRAModel(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim):\n",
        "        super(LoRAModel, self).__init__()\n",
        "        self.fc = nn.Linear(input_dim, hidden_dim)\n",
        "        self.lora_A = nn.Parameter(torch.randn(hidden_dim, 4))  # Low-rank matrix A\n",
        "        self.lora_B = nn.Parameter(torch.randn(4, input_dim))   # Low-rank matrix B\n",
        "\n",
        "    def forward(self, x):\n",
        "        W = self.fc.weight\n",
        "        lora_update = torch.matmul(self.lora_A, self.lora_B)\n",
        "        updated_W = W + lora_update\n",
        "        x = torch.matmul(x, updated_W.T) + self.fc.bias\n",
        "        return x\n",
        "\n",
        "# Define quantization functions\n",
        "def quantize_tensor(tensor, num_bits=8):\n",
        "    scale = tensor.abs().max() / (2 ** (num_bits - 1) - 1)\n",
        "    quantized = torch.round(tensor / scale).int()\n",
        "    return quantized, scale\n",
        "\n",
        "def dequantize_tensor(quantized, scale):\n",
        "    return quantized.float() * scale\n",
        "\n",
        "# Instantiate the model\n",
        "input_dim = 10\n",
        "hidden_dim = 6\n",
        "model = LoRAModel(input_dim, hidden_dim)\n",
        "\n",
        "# Apply LoRA\n",
        "lora_A = model.lora_A\n",
        "lora_B = model.lora_B\n",
        "W = model.fc.weight\n",
        "lora_update = torch.matmul(lora_A, lora_B)\n",
        "updated_W_lora = W + lora_update\n",
        "\n",
        "# Apply QLoRA\n",
        "quantized_A, scale_A = quantize_tensor(lora_A)\n",
        "quantized_B, scale_B = quantize_tensor(lora_B)\n",
        "dequantized_A = dequantize_tensor(quantized_A, scale_A)\n",
        "dequantized_B = dequantize_tensor(quantized_B, scale_B)\n",
        "lora_update_quantized = torch.matmul(dequantized_A, dequantized_B)\n",
        "updated_W_qlora = W + lora_update_quantized\n",
        "\n",
        "# Print the differences between LoRA and QLoRA\n",
        "print(\"Original Weight Matrix W:\\n\", W)\n",
        "print(\"\\nLow-Rank Update ΔW (LoRA):\\n\", lora_update)\n",
        "print(\"\\nUpdated Weight Matrix W' (LoRA):\\n\", updated_W_lora)\n",
        "print(\"\\nLow-Rank Update ΔW (QLoRA):\\n\", lora_update_quantized)\n",
        "print(\"\\nUpdated Weight Matrix W' (QLoRA):\\n\", updated_W_qlora)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TnE3CjdXV7-G",
        "outputId": "bf1aab94-8798-448d-a9bd-30a6096b1444"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Weight Matrix W:\n",
            " Parameter containing:\n",
            "tensor([[ 0.2674,  0.1404, -0.2016,  0.0831, -0.2406, -0.0549,  0.2625,  0.2510,\n",
            "         -0.1550, -0.1925],\n",
            "        [-0.2031, -0.0693, -0.1186,  0.2365,  0.0907, -0.3049,  0.2085, -0.2339,\n",
            "         -0.0290,  0.0925],\n",
            "        [ 0.1454,  0.0029,  0.1722, -0.0569,  0.1638,  0.1947,  0.0819, -0.0532,\n",
            "         -0.1233, -0.3128],\n",
            "        [ 0.1417,  0.1994,  0.2274,  0.0310,  0.2003,  0.2961,  0.0400,  0.1053,\n",
            "         -0.0526, -0.0381],\n",
            "        [ 0.1214,  0.1669,  0.0250, -0.2070,  0.0904, -0.0866,  0.2056, -0.1328,\n",
            "          0.1986,  0.2691],\n",
            "        [ 0.2832, -0.0298,  0.1418,  0.2959,  0.0232,  0.0285,  0.2896, -0.2381,\n",
            "         -0.0884, -0.0879]], requires_grad=True)\n",
            "\n",
            "Low-Rank Update ΔW (LoRA):\n",
            " tensor([[ 0.6609,  0.3984, -0.8163,  1.0553,  0.0283,  0.9763,  0.9529,  3.1155,\n",
            "          1.9216,  1.0668],\n",
            "        [-1.2604, -2.4124, -0.4593,  0.0493, -0.1857,  1.7256, -0.6232,  1.3840,\n",
            "          0.6693,  2.9433],\n",
            "        [ 1.2750,  0.8068,  0.4337, -0.2103,  0.4962, -1.1173,  1.1757,  0.1960,\n",
            "          0.8139, -1.4456],\n",
            "        [ 2.7693,  2.8675,  0.5078,  0.1884,  0.7960, -2.1777,  1.5000, -0.4641,\n",
            "          0.6032, -3.7816],\n",
            "        [-0.5173,  2.3233, -0.0839,  0.2477, -0.7534, -0.6304,  0.2822, -0.5692,\n",
            "         -1.2192, -1.2063],\n",
            "        [ 2.5228,  5.0574, -0.0179,  1.0279,  0.1761, -2.1396, -0.1067, -2.4044,\n",
            "         -1.7260, -5.2096]], grad_fn=<MmBackward0>)\n",
            "\n",
            "Updated Weight Matrix W' (LoRA):\n",
            " tensor([[ 0.9283,  0.5388, -1.0179,  1.1383, -0.2123,  0.9214,  1.2153,  3.3665,\n",
            "          1.7666,  0.8743],\n",
            "        [-1.4635, -2.4817, -0.5780,  0.2858, -0.0950,  1.4208, -0.4147,  1.1501,\n",
            "          0.6402,  3.0357],\n",
            "        [ 1.4204,  0.8096,  0.6059, -0.2672,  0.6600, -0.9226,  1.2576,  0.1428,\n",
            "          0.6907, -1.7584],\n",
            "        [ 2.9110,  3.0669,  0.7352,  0.2195,  0.9962, -1.8816,  1.5400, -0.3588,\n",
            "          0.5505, -3.8197],\n",
            "        [-0.3959,  2.4901, -0.0590,  0.0407, -0.6630, -0.7169,  0.4878, -0.7020,\n",
            "         -1.0206, -0.9373],\n",
            "        [ 2.8060,  5.0275,  0.1238,  1.3238,  0.1993, -2.1111,  0.1828, -2.6424,\n",
            "         -1.8144, -5.2975]], grad_fn=<AddBackward0>)\n",
            "\n",
            "Low-Rank Update ΔW (QLoRA):\n",
            " tensor([[ 0.6621,  0.3962, -0.8154,  1.0617,  0.0340,  0.9653,  0.9758,  3.1407,\n",
            "          1.9280,  1.0749],\n",
            "        [-1.2398, -2.3943, -0.4676,  0.0408, -0.1826,  1.7199, -0.6236,  1.3763,\n",
            "          0.6700,  2.9325],\n",
            "        [ 1.2751,  0.8099,  0.4426, -0.1965,  0.4977, -1.1287,  1.1748,  0.1979,\n",
            "          0.8072, -1.4572],\n",
            "        [ 2.7452,  2.8258,  0.5238,  0.2071,  0.8006, -2.1762,  1.4967, -0.4508,\n",
            "          0.6007, -3.7603],\n",
            "        [-0.5137,  2.3374, -0.0867,  0.2521, -0.7553, -0.6260,  0.2927, -0.5606,\n",
            "         -1.2116, -1.2190],\n",
            "        [ 2.4978,  5.0547, -0.0095,  1.0494,  0.1752, -2.1372, -0.0880, -2.3911,\n",
            "         -1.7280, -5.2096]], grad_fn=<MmBackward0>)\n",
            "\n",
            "Updated Weight Matrix W' (QLoRA):\n",
            " tensor([[ 0.9295,  0.5366, -1.0170,  1.1448, -0.2066,  0.9104,  1.2383,  3.3916,\n",
            "          1.7730,  0.8824],\n",
            "        [-1.4429, -2.4636, -0.5863,  0.2774, -0.0919,  1.4150, -0.4151,  1.1424,\n",
            "          0.6409,  3.0250],\n",
            "        [ 1.4206,  0.8127,  0.6148, -0.2535,  0.6615, -0.9340,  1.2567,  0.1447,\n",
            "          0.6840, -1.7700],\n",
            "        [ 2.8869,  3.0252,  0.7512,  0.2381,  1.0009, -1.8801,  1.5367, -0.3455,\n",
            "          0.5481, -3.7984],\n",
            "        [-0.3923,  2.5043, -0.0617,  0.0451, -0.6650, -0.7125,  0.4983, -0.6935,\n",
            "         -1.0130, -0.9499],\n",
            "        [ 2.7810,  5.0249,  0.1323,  1.3453,  0.1984, -2.1087,  0.2016, -2.6292,\n",
            "         -1.8165, -5.2975]], grad_fn=<AddBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below is the code to demonstrate the overall size of the training parameters for LoRA and QLoRA. We'll measure the size of the model parameters in memory before and after quantization.\n",
        "\n",
        "```python\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import os\n",
        "\n",
        "# Define a simple model with LoRA applied to one layer\n",
        "class LoRAModel(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim):\n",
        "        super(LoRAModel, self).__init__()\n",
        "        self.fc = nn.Linear(input_dim, hidden_dim)\n",
        "        self.lora_A = nn.Parameter(torch.randn(hidden_dim, 4))  # Low-rank matrix A\n",
        "        self.lora_B = nn.Parameter(torch.randn(4, input_dim))   # Low-rank matrix B\n",
        "    \n",
        "    def forward(self, x):\n",
        "        W = self.fc.weight\n",
        "        lora_update = torch.matmul(self.lora_A, self.lora_B)\n",
        "        updated_W = W + lora_update\n",
        "        x = torch.matmul(x, updated_W.T) + self.fc.bias\n",
        "        return x\n",
        "\n",
        "# Define quantization functions\n",
        "def quantize_tensor(tensor, num_bits=8):\n",
        "    scale = tensor.abs().max() / (2 ** (num_bits - 1) - 1)\n",
        "    quantized = torch.round(tensor / scale).int()\n",
        "    return quantized, scale\n",
        "\n",
        "def dequantize_tensor(quantized, scale):\n",
        "    return quantized.float() * scale\n",
        "\n",
        "# Function to calculate the size of the model parameters in MB\n",
        "def get_model_size(model):\n",
        "    torch.save(model.state_dict(), \"temp.p\")\n",
        "    size = os.path.getsize(\"temp.p\") / 1e6\n",
        "    os.remove(\"temp.p\")\n",
        "    return size\n",
        "\n",
        "# Instantiate the model\n",
        "input_dim = 10\n",
        "hidden_dim = 6\n",
        "model = LoRAModel(input_dim, hidden_dim)\n",
        "\n",
        "# Size of the original model with LoRA\n",
        "original_model_size = get_model_size(model)\n",
        "print(f\"Original model size (with LoRA): {original_model_size:.4f} MB\")\n",
        "\n",
        "# Apply QLoRA\n",
        "quantized_A, scale_A = quantize_tensor(model.lora_A)\n",
        "quantized_B, scale_B = quantize_tensor(model.lora_B)\n",
        "dequantized_A = dequantize_tensor(quantized_A, scale_A)\n",
        "dequantized_B = dequantize_tensor(quantized_B, scale_B)\n",
        "\n",
        "# Replace the original low-rank matrices with quantized versions\n",
        "model.lora_A.data = dequantized_A\n",
        "model.lora_B.data = dequantized_B\n",
        "\n",
        "# Size of the model with QLoRA\n",
        "quantized_model_size = get_model_size(model)\n",
        "print(f\"Quantized model size (with QLoRA): {quantized_model_size:.4f} MB\")\n",
        "\n",
        "# Summary of sizes\n",
        "print(f\"Size reduction: {original_model_size - quantized_model_size:.4f} MB\")\n",
        "```\n",
        "\n",
        "### Explanation\n",
        "\n",
        "1. **Model Definition**:\n",
        "   - `LoRAModel`: A simple model with a linear layer and low-rank adaptation matrices `A` and `B`.\n",
        "\n",
        "2. **Quantization Functions**:\n",
        "   - `quantize_tensor`: Quantizes a tensor to a specified number of bits.\n",
        "   - `dequantize_tensor`: Converts a quantized tensor back to its original floating-point representation.\n",
        "\n",
        "3. **Model Size Calculation**:\n",
        "   - `get_model_size`: Saves the model's state dictionary to a temporary file and calculates its size in megabytes (MB).\n",
        "\n",
        "4. **Applying LoRA and QLoRA**:\n",
        "   - The original model size is measured after initializing the model with LoRA.\n",
        "   - Low-rank matrices `A` and `B` are quantized and then dequantized to simulate the effect of quantization.\n",
        "   - The quantized matrices are assigned back to the model.\n",
        "   - The size of the model with QLoRA is measured after replacing the original matrices with their quantized versions.\n",
        "\n",
        "5. **Printing the Results**:\n",
        "   - The original model size and quantized model size are printed along with the size reduction achieved by quantization.\n",
        "\n",
        "This code provides a clear demonstration of the memory size difference between LoRA and QLoRA, highlighting the efficiency gained through quantization."
      ],
      "metadata": {
        "id": "Ej41duxOXTNd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import os\n",
        "\n",
        "# Define a simple model with LoRA applied to one layer\n",
        "class LoRAModel(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim):\n",
        "        super(LoRAModel, self).__init__()\n",
        "        self.fc = nn.Linear(input_dim, hidden_dim)\n",
        "        self.lora_A = nn.Parameter(torch.randn(hidden_dim, 4))  # Low-rank matrix A\n",
        "        self.lora_B = nn.Parameter(torch.randn(4, input_dim))   # Low-rank matrix B\n",
        "\n",
        "    def forward(self, x):\n",
        "        W = self.fc.weight\n",
        "        lora_update = torch.matmul(self.lora_A, self.lora_B)\n",
        "        updated_W = W + lora_update\n",
        "        x = torch.matmul(x, updated_W.T) + self.fc.bias\n",
        "        return x\n",
        "\n",
        "# Define quantization functions\n",
        "def quantize_tensor(tensor, num_bits=8):\n",
        "    scale = tensor.abs().max() / (2 ** (num_bits - 1) - 1)\n",
        "    quantized = torch.round(tensor / scale).int()\n",
        "    return quantized, scale\n",
        "\n",
        "def dequantize_tensor(quantized, scale):\n",
        "    return quantized.float() * scale\n",
        "\n",
        "# Function to calculate the size of the model parameters in MB\n",
        "def get_model_size(model):\n",
        "    torch.save(model.state_dict(), \"temp.p\")\n",
        "    size = os.path.getsize(\"temp.p\") / 1e6\n",
        "    os.remove(\"temp.p\")\n",
        "    return size\n",
        "\n",
        "# Instantiate the model\n",
        "input_dim = 10\n",
        "hidden_dim = 6\n",
        "model = LoRAModel(input_dim, hidden_dim)\n",
        "\n",
        "# Size of the original model with LoRA\n",
        "original_model_size = get_model_size(model)\n",
        "print(f\"Original model size (with LoRA): {original_model_size:.4f} MB\")\n",
        "\n",
        "# Apply QLoRA\n",
        "quantized_A, scale_A = quantize_tensor(model.lora_A)\n",
        "quantized_B, scale_B = quantize_tensor(model.lora_B)\n",
        "dequantized_A = dequantize_tensor(quantized_A, scale_A)\n",
        "dequantized_B = dequantize_tensor(quantized_B, scale_B)\n",
        "\n",
        "# Replace the original low-rank matrices with quantized versions\n",
        "model.lora_A.data = dequantized_A\n",
        "model.lora_B.data = dequantized_B\n",
        "\n",
        "# Size of the model with QLoRA\n",
        "quantized_model_size = get_model_size(model)\n",
        "print(f\"Quantized model size (with QLoRA): {quantized_model_size:.4f} MB\")\n",
        "\n",
        "# Summary of sizes\n",
        "print(f\"Size reduction: {original_model_size - quantized_model_size:.4f} MB\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5HIVTIVXXXPA",
        "outputId": "b3fff893-214f-4de5-a17b-cddec497f522"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original model size (with LoRA): 0.0024 MB\n",
            "Quantized model size (with QLoRA): 0.0024 MB\n",
            "Size reduction: 0.0000 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "eVD_NSdokA2m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ced273f4-63e4-440f-8058-aca55a7f7230"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/ao/quantization/observer.py:220: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Static Quantization:\n",
            " SimpleNN(\n",
            "  (fc1): QuantizedLinearReLU(in_features=10, out_features=10, scale=0.009280134923756123, zero_point=0, qscheme=torch.per_channel_affine)\n",
            "  (relu): Identity()\n",
            "  (fc2): QuantizedLinear(in_features=10, out_features=1, scale=0.0022189663723111153, zero_point=127, qscheme=torch.per_channel_affine)\n",
            ")\n",
            "Dynamic Quantization:\n",
            " SimpleNN(\n",
            "  (fc1): DynamicQuantizedLinear(in_features=10, out_features=10, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
            "  (relu): ReLU()\n",
            "  (fc2): DynamicQuantizedLinear(in_features=10, out_features=1, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
            ")\n",
            "Original model size: 0.002424 MB\n",
            "Static quantized model size: 0.004546 MB\n",
            "Dynamic quantized model size: 0.00342 MB\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.quantization as quantization\n",
        "\n",
        "# Define a simple neural network\n",
        "class SimpleNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(10, 10)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(10, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Instantiate the model\n",
        "model = SimpleNN()\n",
        "\n",
        "# Example input\n",
        "example_input = torch.randn(1, 10)\n",
        "\n",
        "# Static Quantization\n",
        "def static_quantization(model, example_input):\n",
        "    # Prepare the model for static quantization\n",
        "    model.qconfig = quantization.get_default_qconfig('fbgemm')\n",
        "    model_fused = quantization.fuse_modules(model, [['fc1', 'relu']])\n",
        "    model_prepared = quantization.prepare(model_fused)\n",
        "\n",
        "    # Calibrate the model with example data\n",
        "    model_prepared(example_input)\n",
        "\n",
        "    # Convert to a quantized model\n",
        "    quantized_model = quantization.convert(model_prepared)\n",
        "    return quantized_model\n",
        "\n",
        "# Dynamic Quantization\n",
        "def dynamic_quantization(model):\n",
        "    # Convert to a dynamically quantized model\n",
        "    quantized_model = quantization.quantize_dynamic(\n",
        "        model, {nn.Linear}, dtype=torch.qint8\n",
        "    )\n",
        "    return quantized_model\n",
        "\n",
        "# Apply static quantization\n",
        "quantized_model_static = static_quantization(model, example_input)\n",
        "print(\"Static Quantization:\\n\", quantized_model_static)\n",
        "\n",
        "# Apply dynamic quantization\n",
        "quantized_model_dynamic = dynamic_quantization(model)\n",
        "print(\"Dynamic Quantization:\\n\", quantized_model_dynamic)\n",
        "\n",
        "# Compare model sizes\n",
        "def get_model_size(model):\n",
        "    torch.save(model.state_dict(), \"temp.p\")\n",
        "    size = os.path.getsize(\"temp.p\") / 1e6\n",
        "    os.remove(\"temp.p\")\n",
        "    return size\n",
        "\n",
        "print(f\"Original model size: {get_model_size(model)} MB\")\n",
        "print(f\"Static quantized model size: {get_model_size(quantized_model_static)} MB\")\n",
        "print(f\"Dynamic quantized model size: {get_model_size(quantized_model_dynamic)} MB\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HJdNYyPJKl8E"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}