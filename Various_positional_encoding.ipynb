{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyODgCAPIycynzm807meBbzA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sharma-himanshukumar/LLM_Learning/blob/main/Various_positional_encoding.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Documentation and When to Use Each Method\n",
        "\n",
        "#### 1. Sinusoidal Positional Encoding\n",
        "\n",
        "**Description**:\n",
        "Sinusoidal positional encodings use sine and cosine functions to encode positions. Each dimension of the encoding corresponds to a different frequency.\n",
        "\n",
        "**Features**:\n",
        "- Fixed and deterministic.\n",
        "- Provides unique encoding for each position.\n",
        "- Ensures smooth transitions between positions, which can help in capturing the order of tokens.\n",
        "\n",
        "**When to Use**:\n",
        "- When you want a simple, fixed positional encoding.\n",
        "- Suitable for applications where the sequence length can vary and you want to avoid learnable parameters for positional encodings.\n",
        "\n",
        "**Code**:\n",
        "```python\n",
        "def get_sinusoidal_positional_encoding(seq_len, d_model):\n",
        "    PE = np.zeros((seq_len, d_model))\n",
        "    for pos in range(seq_len):\n",
        "        for i in range(0, d_model, 2):\n",
        "            PE[pos, i] = np.sin(pos / (10000 ** (i / d_model)))\n",
        "            PE[pos, i + 1] = np.cos(pos / (10000 ** ((i + 1) / d_model)))\n",
        "    return PE\n",
        "```\n",
        "\n",
        "#### 2. Learnable Positional Encoding\n",
        "\n",
        "**Description**:\n",
        "Learnable positional encodings assign a unique embedding vector to each position, which is learned during training.\n",
        "\n",
        "**Features**:\n",
        "- Adaptable to the specific task and data.\n",
        "- Can capture more nuanced positional information.\n",
        "\n",
        "**When to Use**:\n",
        "- When you have enough data and computational resources to train the positional encodings.\n",
        "- Useful for tasks where the model benefits from learning the positional encodings from the data.\n",
        "\n",
        "**Code**:\n",
        "```python\n",
        "class LearnablePositionalEncoding(nn.Module):\n",
        "    def __init__(self, seq_len, d_model):\n",
        "        super().__init__()\n",
        "        self.positional_embeddings = nn.Embedding(seq_len, d_model)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        positions = torch.arange(x.size(1), device=x.device).unsqueeze(0)\n",
        "        return self.positional_embeddings(positions)\n",
        "```\n",
        "\n",
        "#### 3. Relative Positional Encoding\n",
        "\n",
        "**Description**:\n",
        "Relative positional encodings encode the relative distance between tokens instead of their absolute positions.\n",
        "\n",
        "**Features**:\n",
        "- Focuses on the relative position of tokens, which can be more important in some tasks.\n",
        "- Allows for more flexible handling of varying sequence lengths.\n",
        "\n",
        "**When to Use**:\n",
        "- When relative positions are more critical than absolute positions (e.g., in some language modeling tasks).\n",
        "- In models that need to handle sequences of varying lengths dynamically.\n",
        "\n",
        "**Code**:\n",
        "```python\n",
        "def get_relative_positional_encoding(seq_len, d_model):\n",
        "    range_vec = torch.arange(seq_len)\n",
        "    distance_mat = range_vec[None, :] - range_vec[:, None]\n",
        "    relative_positions = F.embedding(distance_mat, torch.randn(2 * seq_len - 1, d_model))\n",
        "    return relative_positions\n",
        "```\n",
        "\n",
        "#### 4. Rotary Positional Embeddings (RoPE)\n",
        "\n",
        "**Description**:\n",
        "Rotary positional embeddings apply a rotation to the query and key vectors based on their positions during the attention mechanism.\n",
        "\n",
        "**Features**:\n",
        "- Integrates positional information directly into the attention mechanism.\n",
        "- Ensures that the positional encoding is inherently tied to the model's computation.\n",
        "\n",
        "**When to Use**:\n",
        "- When you want to tightly couple positional information with the attention mechanism.\n",
        "- Suitable for models that benefit from rotational invariance in positional encodings.\n",
        "\n",
        "**Code**:\n",
        "```python\n",
        "def apply_rope(x, seq_len, d_model):\n",
        "    pos = torch.arange(seq_len, dtype=torch.float32, device=x.device).unsqueeze(1)\n",
        "    freqs = torch.arange(0, d_model, 2, dtype=torch.float32, device=x.device) / d_model\n",
        "    angles = pos * (10000 ** -freqs)\n",
        "    angles = torch.cat([torch.sin(angles), torch.cos(angles)], dim=1)\n",
        "    return x * angles\n",
        "```\n",
        "\n",
        "### Example Usage and Output\n",
        "\n",
        "This script demonstrates how to generate and print positional encodings for a sample input sequence of length 10 with a model dimension of 16. The output shows the different encodings generated by each method, providing a visual comparison of their structures.\n"
      ],
      "metadata": {
        "id": "_C-4G4T6G6h0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AZx_jZhMEXLl",
        "outputId": "b76e8728-b537-411b-e4f8-a779a8f6d7d8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sinusoidal Positional Encoding:\n",
            " [[ 0.00000000e+00  1.00000000e+00  0.00000000e+00  1.00000000e+00\n",
            "   0.00000000e+00  1.00000000e+00  0.00000000e+00  1.00000000e+00\n",
            "   0.00000000e+00  1.00000000e+00  0.00000000e+00  1.00000000e+00\n",
            "   0.00000000e+00  1.00000000e+00  0.00000000e+00  1.00000000e+00]\n",
            " [ 8.41470985e-01  8.46009110e-01  3.10983593e-01  9.84230234e-01\n",
            "   9.98334166e-02  9.98419278e-01  3.16175064e-02  9.99841890e-01\n",
            "   9.99983333e-03  9.99984189e-01  3.16227239e-03  9.99998419e-01\n",
            "   9.99999833e-04  9.99999842e-01  3.16227761e-04  9.99999984e-01]\n",
            " [ 9.09297427e-01  4.31462829e-01  5.91127117e-01  9.37418309e-01\n",
            "   1.98669331e-01  9.93682109e-01  6.32033979e-02  9.99367611e-01\n",
            "   1.99986667e-02  9.99936755e-01  6.32451316e-03  9.99993675e-01\n",
            "   1.99999867e-03  9.99999368e-01  6.32455490e-04  9.99999937e-01]\n",
            " [ 1.41120008e-01 -1.15966142e-01  8.12648897e-01  8.61040649e-01\n",
            "   2.95520207e-01  9.85803469e-01  9.47260913e-02  9.98577313e-01\n",
            "   2.99955002e-02  9.99857701e-01  9.48669068e-03  9.99985770e-01\n",
            "   2.99999550e-03  9.99998577e-01  9.48683156e-04  9.99999858e-01]\n",
            " [-7.56802495e-01 -6.27679654e-01  9.53580740e-01  7.57506172e-01\n",
            "   3.89418342e-01  9.74808266e-01  1.26154067e-01  9.97471244e-01\n",
            "   3.99893342e-02  9.99747028e-01  1.26487733e-02  9.99974702e-01\n",
            "   3.99998933e-03  9.99997470e-01  1.26491073e-03  9.99999747e-01]\n",
            " [-9.58924275e-01 -9.46079269e-01  9.99946517e-01  6.30080304e-01\n",
            "   4.79425539e-01  9.60731261e-01  1.57455898e-01  9.96049756e-01\n",
            "   4.99791693e-02  9.99604741e-01  1.58107295e-02  9.99960472e-01\n",
            "   4.99997917e-03  9.99996047e-01  1.58113817e-03  9.99999605e-01]\n",
            " [-2.79415498e-01 -9.73103708e-01  9.47148158e-01  4.82782000e-01\n",
            "   5.64642473e-01  9.43616957e-01  1.88600287e-01  9.94313298e-01\n",
            "   5.99640065e-02  9.99430844e-01  1.89725276e-02  9.99943080e-01\n",
            "   5.99996400e-03  9.99994308e-01  1.89736546e-03  9.99999431e-01]\n",
            " [ 6.56986599e-01 -7.00429935e-01  8.00421646e-01  3.20256978e-01\n",
            "   6.44217687e-01  9.23519461e-01  2.19556091e-01  9.92262419e-01\n",
            "   6.99428473e-02  9.99225342e-01  2.21341359e-02  9.99922525e-01\n",
            "   6.99994283e-03  9.99992252e-01  2.21359255e-03  9.99999225e-01]\n",
            " [ 9.89358247e-01 -2.12036505e-01  5.74317769e-01  1.47631200e-01\n",
            "   7.17356091e-01  9.00502310e-01  2.50292358e-01  9.89897767e-01\n",
            "   7.99146940e-02  9.98988242e-01  2.52955229e-02  9.99898809e-01\n",
            "   7.99991467e-03  9.99989881e-01  2.52981943e-03  9.99998988e-01]\n",
            " [ 4.12118485e-01  3.41660306e-01  2.91259121e-01 -2.96507958e-02\n",
            "   7.83326910e-01  8.74638270e-01  2.80778353e-01  9.87220090e-01\n",
            "   8.98785492e-02  9.98719551e-01  2.84566569e-02  9.99871930e-01\n",
            "   8.99987850e-03  9.99987193e-01  2.84604605e-03  9.99998719e-01]]\n",
            "Learnable Positional Encoding:\n",
            " tensor([[[-1.5542e-01,  1.2158e-01, -2.1159e-01,  7.0021e-01,  2.4153e-01,\n",
            "          -1.1194e+00,  1.1785e+00,  1.3798e-01,  3.2861e-01, -1.4462e+00,\n",
            "          -1.7967e+00,  1.2640e+00, -1.5751e+00, -5.9911e-02, -1.1007e+00,\n",
            "          -2.8290e-01],\n",
            "         [ 1.0998e+00,  1.7675e+00,  4.6065e-01,  6.2349e-01, -1.7276e-01,\n",
            "           7.3388e-01,  7.9394e-01,  6.7695e-02,  7.3988e-01,  1.9910e+00,\n",
            "          -3.0738e-01,  2.3079e-01,  1.0059e+00,  1.0885e+00, -1.4568e+00,\n",
            "           1.3256e+00],\n",
            "         [ 1.6743e+00,  8.9981e-02,  1.3919e+00,  1.0395e+00,  1.3542e+00,\n",
            "           7.0532e-01,  4.3196e-01,  1.5213e+00,  6.8783e-01,  4.8365e-01,\n",
            "          -1.0792e+00, -3.3297e-01, -1.3795e+00,  2.4397e+00, -4.5616e-01,\n",
            "           2.2532e+00],\n",
            "         [-2.2168e-01,  3.7737e-01,  1.8441e-01, -1.9055e+00,  5.2798e-01,\n",
            "           3.6281e-01, -1.4236e+00,  1.4220e+00,  1.0015e+00,  6.3435e-01,\n",
            "          -2.4454e+00, -7.1577e-02, -1.2489e-03, -1.5440e+00,  8.1736e-01,\n",
            "           6.1380e-01],\n",
            "         [-6.6151e-01, -5.6611e-01, -9.6842e-01,  1.4882e-02,  1.1630e+00,\n",
            "           1.1115e+00,  7.4761e-01, -1.6062e+00,  2.7086e-02,  1.6145e+00,\n",
            "          -1.6853e+00, -3.0394e-01,  2.2797e-01, -6.3108e-01, -6.2726e-01,\n",
            "          -7.5166e-01],\n",
            "         [ 9.8567e-02, -7.8271e-01,  1.0914e+00,  1.2407e+00, -7.2787e-01,\n",
            "           1.6461e+00,  1.9991e-01, -3.4436e-01,  8.5645e-01, -1.0486e+00,\n",
            "           3.3828e-01,  1.1278e+00, -8.1558e-01,  3.7111e-01, -5.3285e-01,\n",
            "           1.2580e-01],\n",
            "         [-1.4244e-01, -3.2300e-01, -9.2878e-01, -1.2298e+00,  4.9007e-01,\n",
            "          -2.0952e+00, -5.7037e-02,  7.5068e-01, -9.7469e-02, -5.3948e-01,\n",
            "           1.8198e+00,  5.9133e-01,  6.1546e-01,  1.5542e-01,  8.4525e-02,\n",
            "          -1.1816e+00],\n",
            "         [-1.2000e+00,  8.2211e-03,  4.3191e-01, -6.4988e-01,  1.6019e-01,\n",
            "           3.3927e-01, -1.2700e+00, -1.1121e+00,  9.5880e-01, -1.3718e+00,\n",
            "           1.9425e+00,  2.8472e-01, -3.8724e-01,  5.7895e-01, -4.1622e-01,\n",
            "           6.6565e-01],\n",
            "         [-7.8370e-01, -4.7935e-01,  6.0193e-01, -1.3456e+00, -8.1423e-01,\n",
            "          -1.2570e+00, -5.3630e-01,  2.2194e-01, -8.6937e-01,  9.5160e-01,\n",
            "          -4.4959e-01,  4.2532e-03, -7.0977e-01, -2.7598e-01,  7.8868e-01,\n",
            "          -3.4712e-01],\n",
            "         [-1.0776e-01,  7.6916e-01, -5.8974e-01, -5.1708e-01, -1.2905e-01,\n",
            "           1.7945e-01, -5.8339e-01, -1.3917e+00,  2.0740e+00, -1.5253e+00,\n",
            "           3.4727e-01,  5.1852e-01,  1.5039e+00,  6.2499e-01,  1.9611e-01,\n",
            "           2.5176e+00]]], grad_fn=<EmbeddingBackward0>)\n",
            "Relative Positional Encoding:\n",
            " tensor([[[-9.9967e-01,  1.1223e+00,  7.8313e-01,  ..., -6.0089e-01,\n",
            "          -2.6319e-01, -6.9515e-01],\n",
            "         [-9.8909e-01,  6.8103e-01, -1.7588e-02,  ..., -1.9334e+00,\n",
            "          -1.0259e+00, -7.6716e-01],\n",
            "         [ 5.7898e-01,  4.0734e-01,  1.9090e+00,  ...,  7.5517e-01,\n",
            "          -1.2380e+00, -2.8568e-01],\n",
            "         ...,\n",
            "         [-5.3879e-01, -3.8216e-02,  1.8674e+00,  ..., -1.5497e-01,\n",
            "          -2.1047e-01, -4.3430e-01],\n",
            "         [-1.1035e+00,  1.4193e+00, -2.9417e-01,  ..., -6.2104e-01,\n",
            "           4.1058e-01,  1.1169e+00],\n",
            "         [-1.7725e-03,  3.6029e-01,  1.1933e+00,  ..., -4.3785e-01,\n",
            "          -1.2576e+00, -1.3267e+00]],\n",
            "\n",
            "        [[-2.1584e+00,  7.1509e-01, -1.0594e+00,  ...,  1.1895e+00,\n",
            "           3.6564e-01,  1.2806e-02],\n",
            "         [-9.9967e-01,  1.1223e+00,  7.8313e-01,  ..., -6.0089e-01,\n",
            "          -2.6319e-01, -6.9515e-01],\n",
            "         [-9.8909e-01,  6.8103e-01, -1.7588e-02,  ..., -1.9334e+00,\n",
            "          -1.0259e+00, -7.6716e-01],\n",
            "         ...,\n",
            "         [-8.7160e-01, -1.7092e-01,  3.3411e-01,  ..., -1.6355e+00,\n",
            "           7.5902e-01, -7.4981e-01],\n",
            "         [-5.3879e-01, -3.8216e-02,  1.8674e+00,  ..., -1.5497e-01,\n",
            "          -2.1047e-01, -4.3430e-01],\n",
            "         [-1.1035e+00,  1.4193e+00, -2.9417e-01,  ..., -6.2104e-01,\n",
            "           4.1058e-01,  1.1169e+00]],\n",
            "\n",
            "        [[ 4.1779e-01,  4.5794e-01,  5.1655e-02,  ..., -9.9158e-01,\n",
            "           2.5333e-01,  6.2138e-01],\n",
            "         [-2.1584e+00,  7.1509e-01, -1.0594e+00,  ...,  1.1895e+00,\n",
            "           3.6564e-01,  1.2806e-02],\n",
            "         [-9.9967e-01,  1.1223e+00,  7.8313e-01,  ..., -6.0089e-01,\n",
            "          -2.6319e-01, -6.9515e-01],\n",
            "         ...,\n",
            "         [ 1.9997e-01,  1.1200e+00,  7.2055e-01,  ..., -6.6563e-01,\n",
            "          -2.7485e-01,  1.1481e-01],\n",
            "         [-8.7160e-01, -1.7092e-01,  3.3411e-01,  ..., -1.6355e+00,\n",
            "           7.5902e-01, -7.4981e-01],\n",
            "         [-5.3879e-01, -3.8216e-02,  1.8674e+00,  ..., -1.5497e-01,\n",
            "          -2.1047e-01, -4.3430e-01]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[-1.0729e-01,  8.2422e-01, -3.3539e-01,  ..., -1.0822e+00,\n",
            "          -8.0802e-01,  2.1625e-01],\n",
            "         [ 5.2780e-01,  1.5605e-01,  1.2167e-02,  ..., -7.6312e-01,\n",
            "           4.7149e-01,  1.1563e-01],\n",
            "         [-8.3970e-01,  8.2892e-01, -2.2499e-01,  ...,  5.9564e-01,\n",
            "          -1.2799e+00,  6.0106e-02],\n",
            "         ...,\n",
            "         [-9.9967e-01,  1.1223e+00,  7.8313e-01,  ..., -6.0089e-01,\n",
            "          -2.6319e-01, -6.9515e-01],\n",
            "         [-9.8909e-01,  6.8103e-01, -1.7588e-02,  ..., -1.9334e+00,\n",
            "          -1.0259e+00, -7.6716e-01],\n",
            "         [ 5.7898e-01,  4.0734e-01,  1.9090e+00,  ...,  7.5517e-01,\n",
            "          -1.2380e+00, -2.8568e-01]],\n",
            "\n",
            "        [[ 3.9581e-01,  7.6327e-01,  2.1919e+00,  ...,  4.4207e-01,\n",
            "           1.6403e+00, -2.4251e-01],\n",
            "         [-1.0729e-01,  8.2422e-01, -3.3539e-01,  ..., -1.0822e+00,\n",
            "          -8.0802e-01,  2.1625e-01],\n",
            "         [ 5.2780e-01,  1.5605e-01,  1.2167e-02,  ..., -7.6312e-01,\n",
            "           4.7149e-01,  1.1563e-01],\n",
            "         ...,\n",
            "         [-2.1584e+00,  7.1509e-01, -1.0594e+00,  ...,  1.1895e+00,\n",
            "           3.6564e-01,  1.2806e-02],\n",
            "         [-9.9967e-01,  1.1223e+00,  7.8313e-01,  ..., -6.0089e-01,\n",
            "          -2.6319e-01, -6.9515e-01],\n",
            "         [-9.8909e-01,  6.8103e-01, -1.7588e-02,  ..., -1.9334e+00,\n",
            "          -1.0259e+00, -7.6716e-01]],\n",
            "\n",
            "        [[-9.0129e-02,  1.5368e+00,  1.4313e-01,  ..., -7.4851e-01,\n",
            "           3.3145e-01,  2.5170e-01],\n",
            "         [ 3.9581e-01,  7.6327e-01,  2.1919e+00,  ...,  4.4207e-01,\n",
            "           1.6403e+00, -2.4251e-01],\n",
            "         [-1.0729e-01,  8.2422e-01, -3.3539e-01,  ..., -1.0822e+00,\n",
            "          -8.0802e-01,  2.1625e-01],\n",
            "         ...,\n",
            "         [ 4.1779e-01,  4.5794e-01,  5.1655e-02,  ..., -9.9158e-01,\n",
            "           2.5333e-01,  6.2138e-01],\n",
            "         [-2.1584e+00,  7.1509e-01, -1.0594e+00,  ...,  1.1895e+00,\n",
            "           3.6564e-01,  1.2806e-02],\n",
            "         [-9.9967e-01,  1.1223e+00,  7.8313e-01,  ..., -6.0089e-01,\n",
            "          -2.6319e-01, -6.9515e-01]]])\n",
            "Rotary Positional Encoding:\n",
            " tensor([[[ 0.0000e+00,  0.0000e+00, -0.0000e+00, -0.0000e+00,  0.0000e+00,\n",
            "          -0.0000e+00,  0.0000e+00,  0.0000e+00, -1.1138e+00, -1.6895e-01,\n",
            "           1.1688e-02,  8.4106e-01, -8.9154e-01, -4.2839e-01,  7.2595e-02,\n",
            "          -3.0541e-01],\n",
            "         [-1.7198e-01, -1.5971e-01, -4.7750e-02,  1.3980e-02, -1.4916e-02,\n",
            "          -4.0227e-03, -3.8545e-04,  1.0961e-04,  9.3134e-02,  7.1484e-01,\n",
            "          -1.6504e+00,  1.6479e+00, -1.1836e+00, -7.5151e-01,  5.4471e-01,\n",
            "           4.4877e-01],\n",
            "         [ 7.9716e-01, -1.3216e+00, -1.7238e-01, -7.4433e-02, -7.1648e-03,\n",
            "           1.3709e-03,  1.2082e-05, -6.0489e-04, -5.0139e-01, -1.8884e+00,\n",
            "          -1.2896e+00,  8.9398e-01, -1.1996e+00,  1.0601e+00, -5.2203e-01,\n",
            "           2.3534e+00],\n",
            "         [-3.1228e-02, -1.3267e-01,  1.8089e-01,  4.8344e-02, -4.1774e-02,\n",
            "           1.5003e-03, -2.9819e-03,  2.2703e-03, -7.1272e-01,  3.6180e-03,\n",
            "          -3.4833e-01, -1.0684e+00,  1.0996e+00,  2.3157e-01,  1.6626e+00,\n",
            "          -1.8566e+00],\n",
            "         [-5.8498e-01, -6.2775e-01, -1.8937e-01,  1.7265e-01, -1.2023e-02,\n",
            "           3.1810e-02,  8.1785e-04,  6.1293e-05,  4.8893e-01, -1.1214e-01,\n",
            "           9.1140e-01,  1.9892e+00,  6.8621e-01, -3.0768e-01, -2.2593e+00,\n",
            "           1.0902e+00],\n",
            "         [ 8.0012e-01,  3.0336e-02, -5.1384e-01,  6.5983e-02,  5.8148e-02,\n",
            "          -1.0884e-02, -1.8648e-03, -1.0783e-03, -2.2890e-03, -4.6351e-03,\n",
            "           1.5126e+00,  2.3622e+00,  8.4187e-01,  4.5137e-01, -2.1767e-01,\n",
            "           3.7798e-01],\n",
            "         [ 1.8360e-01, -4.3875e-01,  8.7833e-01, -3.6163e-01,  1.0912e-01,\n",
            "          -1.6814e-02,  7.4278e-04,  1.9824e-03, -3.9706e-01,  4.6282e-01,\n",
            "           9.2019e-01, -7.2032e-01,  1.8621e+00,  5.4729e-01,  1.3373e+00,\n",
            "          -7.1553e-01],\n",
            "         [-5.6398e-02,  3.1119e-01,  4.1935e-01,  2.3389e-01,  8.2708e-02,\n",
            "           4.9937e-03,  3.7742e-04, -5.7982e-03, -1.7320e-01,  1.1819e-01,\n",
            "           3.3156e-01, -1.5373e+00, -4.3526e-02, -8.6715e-01, -3.2069e-01,\n",
            "          -2.4297e+00],\n",
            "         [-4.8796e-01,  1.3932e-01, -8.6258e-01, -2.1460e-01,  1.8650e-02,\n",
            "           6.1480e-03, -9.1928e-03,  6.3814e-03, -3.2715e-01,  7.9572e-01,\n",
            "          -5.5014e-01,  1.1317e+00, -1.2506e+00, -3.1747e-02,  4.9361e-01,\n",
            "           2.7514e-01],\n",
            "         [ 3.3683e-01,  7.7214e-02,  6.3675e-02,  2.3897e-01, -8.8513e-03,\n",
            "          -3.0101e-02, -6.4751e-04, -5.0738e-03, -1.0724e+00, -5.9089e-01,\n",
            "          -5.5096e-01,  1.3579e+00,  9.7321e-01, -1.1439e-01,  6.5065e-01,\n",
            "          -7.0609e-01]]])\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Sinusoidal Positional Encoding\n",
        "def get_sinusoidal_positional_encoding(seq_len, d_model):\n",
        "    PE = np.zeros((seq_len, d_model))\n",
        "    for pos in range(seq_len):\n",
        "        for i in range(0, d_model, 2):\n",
        "            PE[pos, i] = np.sin(pos / (10000 ** (i / d_model)))\n",
        "            PE[pos, i + 1] = np.cos(pos / (10000 ** ((i + 1) / d_model)))\n",
        "    return PE\n",
        "\n",
        "# Learnable Positional Encoding\n",
        "class LearnablePositionalEncoding(nn.Module):\n",
        "    def __init__(self, seq_len, d_model):\n",
        "        super().__init__()\n",
        "        self.positional_embeddings = nn.Embedding(seq_len, d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        positions = torch.arange(x.size(1), device=x.device).unsqueeze(0)\n",
        "        return self.positional_embeddings(positions)\n",
        "\n",
        "# Relative Positional Encoding (Simplified)\n",
        "def get_relative_positional_encoding(seq_len, d_model):\n",
        "    range_vec = torch.arange(seq_len)\n",
        "    distance_mat = range_vec[None, :] - range_vec[:, None]\n",
        "    distance_mat = distance_mat +seq_len -1\n",
        "    relative_positions = F.embedding(distance_mat, torch.randn(2 * seq_len - 1, d_model))\n",
        "    return relative_positions\n",
        "\n",
        "# Rotary Positional Embeddings (RoPE)\n",
        "def apply_rope(x, seq_len, d_model):\n",
        "    pos = torch.arange(seq_len, dtype=torch.float32, device=x.device).unsqueeze(1)\n",
        "    freqs = torch.arange(0, d_model, 2, dtype=torch.float32, device=x.device) / d_model\n",
        "    angles = pos * (10000 ** -freqs)\n",
        "    angles = torch.cat([torch.sin(angles), torch.cos(angles)], dim=1)\n",
        "    return x * angles\n",
        "\n",
        "# Parameters\n",
        "seq_len = 10\n",
        "d_model = 16\n",
        "\n",
        "# Sample input\n",
        "x = torch.randn(1, seq_len, d_model)\n",
        "\n",
        "# Sinusoidal Positional Encoding\n",
        "sinusoidal_encoding = get_sinusoidal_positional_encoding(seq_len, d_model)\n",
        "print(\"Sinusoidal Positional Encoding:\\n\", sinusoidal_encoding)\n",
        "\n",
        "# Learnable Positional Encoding\n",
        "learnable_pos_enc = LearnablePositionalEncoding(seq_len, d_model)\n",
        "learnable_encoding = learnable_pos_enc(x)\n",
        "print(\"Learnable Positional Encoding:\\n\", learnable_encoding)\n",
        "\n",
        "# Relative Positional Encoding\n",
        "relative_encoding = get_relative_positional_encoding(seq_len, d_model)\n",
        "print(\"Relative Positional Encoding:\\n\", relative_encoding)\n",
        "\n",
        "# Rotary Positional Embeddings (RoPE)\n",
        "rope_encoding = apply_rope(x, seq_len, d_model)\n",
        "print(\"Rotary Positional Encoding:\\n\", rope_encoding)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oboDVfmLG1JQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}